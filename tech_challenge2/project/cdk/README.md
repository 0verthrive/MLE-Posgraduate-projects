# ğŸ“Š Pipeline de Dados B3 com AWS

Este projeto constrÃ³i uma pipeline completa para **extrair, processar e analisar dados do pregÃ£o da B3**, utilizando os serviÃ§os **AWS S3, Glue, Lambda e Athena**.

---

## ğŸš€ Objetivo

Automatizar o processamento de dados do pregÃ£o da B3, organizando os dados em camadas (`raw` e `refined`) e possibilitando consultas analÃ­ticas via Athena.

---

## ğŸ§± Arquitetura

![Arquitetura do Projeto](image.png)

---

## â˜ï¸ ServiÃ§os AWS Utilizados

### **S3**
Armazena os dados em diferentes estÃ¡gios da pipeline:

- **Raw**: dados crus no formato Parquet  
   Estrutura: s3://<bucket-name>/raw/IBOV/yyyy/mm/dd/file.parquet


- **Refined**: dados tratados no formato Parquet, com partiÃ§Ã£o por data  
   Estrutura: s3://<bucket-name>/refined/data=yyyy-mm-dd/file.parquet


- **Glue**: scripts de transformaÃ§Ã£o e arquivos temporÃ¡rios

- **Athena results**: local onde o Athena salva os resultados de consultas

---

### **Lambda**
Monitora o bucket `raw/`. Sempre que um novo arquivo Ã© adicionado, aciona o Glue Job automaticamente.

---

### **Glue**
ResponsÃ¡vel por:

- Renomear colunas  
- Remover campos desnecessÃ¡rios  
- Adicionar e particionar por coluna `data`  
- Salvar os dados refinados no bucket `refined/`

---

### **AWS Glue Data Catalog**
Define a estrutura do banco de dados e tabela que permite a leitura dos dados refinados via Athena.

---

## ğŸ› ï¸ Infraestrutura como CÃ³digo (CDK)

O provisionamento da infraestrutura Ã© feito com [AWS CDK (Cloud Development Kit)](https://docs.aws.amazon.com/cdk/v2/guide/home.html), utilizando Python.

### Estrutura do Projeto

cdk/  
â”œâ”€â”€ cdk_stack.py # Stack principal que orquestra todos os recursos  
â”œâ”€â”€ s3_stack.py # Stack do bucket S3  
â”œâ”€â”€ lambda_stack.py # Stack da funÃ§Ã£o Lambda  
â”œâ”€â”€ glue_stacks.py # Stacks para Glue Database, Table e Job  
â”œâ”€â”€ source/ # Scripts para Lambda e Glue  
â””â”€â”€ config/ # Arquivos YAML de configuraÃ§Ã£o  


---

## ğŸ“‹ PrÃ©-requisitos

- Python 3.8+
- [Node.js](https://nodejs.org/) (recomendado: LTS)
- [AWS CLI](https://aws.amazon.com/cli/) configurado
- [AWS CDK Toolkit](https://docs.aws.amazon.com/cdk/v2/guide/cli.html) instalado:
  ```bash
  npm install -g aws-cdk


âš™ï¸ InstalaÃ§Ã£o e Deploy
1. Clone o repositÃ³rio e instale as dependÃªncias:
```bash
cd cdk
python -m venv .venv

# Linux/Mac
source .venv/bin/activate

# Windows
.venv\Scripts\activate.bat

pip install -r requirements.txt
```

2. Configure o arquivo:  
config/config.yaml

3. Crie e configure:  
config/environments.yaml

4. Gere o template CloudFormation:
```bash
cdk synth
```

5. FaÃ§a o deploy da infraestrutura:
```bash
cdk deploy
```
6. (Opcional) Remova os recursos:
```bash
cdk destroy
```

---
âš ï¸ PermissÃµes
Certifique-se de que o usuÃ¡rio ou role da AWS utilizada possui permissÃµes para criar e gerenciar os seguintes recursos:

- S3

- Glue

- Lambda

- IAM

- Athena

---

## ğŸ“Œ Como Usar a Pipeline

ApÃ³s o deploy da infraestrutura com CDK, a pipeline funcionarÃ¡ de forma **automatizada** com base no seguinte fluxo:

1. **Adicione um novo arquivo Parquet no bucket S3**, na pasta `raw/IBOV/yy/mm/dd/`.
2. A **funÃ§Ã£o Lambda** serÃ¡ acionada automaticamente assim que o arquivo for detectado.
3. A Lambda dispara a execuÃ§Ã£o do **Glue Job**, que:
   - Realiza as transformaÃ§Ãµes nos dados.
   - Adiciona a coluna `data` (caso ainda nÃ£o exista).
   - Salva os dados tratados no diretÃ³rio `refined/`, particionado por data.
4. O **Glue Catalog** Ã© atualizado (caso necessÃ¡rio), e os dados jÃ¡ podem ser consultados via **Athena**.

---

## ğŸ§ª Exemplo de ExecuÃ§Ã£o

### 1. Upload do Arquivo para o Raw

Adicione um arquivo Parquet no seguinte caminho no S3:
   s3://<bucket-name>/refined/data=2025-08-06/dados_b3.parquet

### 4. Consulta no Athena

VocÃª pode executar uma consulta no Athena como:

```sql
SELECT *
FROM b3_catalog_db.refined_ibov
WHERE data = DATE '2025-08-06'
ORDER BY qtde_teorica DESC;

```

